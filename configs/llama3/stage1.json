{
    "model_name_or_path":"LLM-Research/Meta-Llama-3-8B",
    "ptm_model_path":"roberta",
    "author_data":"data/IND-WhoIsWho/train_author.json",
    "pub_data":"data/IND-WhoIsWho/pid_to_info_all.json",
    "eval_data":"data/IND-WhoIsWho/ind_valid_author.json",
    "eval_ground_truth":"data/IND-WhoIsWho/ind_valid_author_ground_truth.json",
    "output_dir":"output/llama3/stage1",
    "run_name":"stage1",
    "preprocessing_num_workers": 20,

    "lora_rank":8,
    "lora_alpha":16,
    "lora_dropout":0.05,

    "enable_llm_requires_grad": true,

    "max_source_length":8192,
    "per_device_train_batch_size":1,
    "per_device_eval_batch_size":1,
    "gradient_accumulation_steps":16,

    "num_train_epochs":6,
    "warmup_ratio":0.1,
    "lr_scheduler_type": "cosine",
    "learning_rate":1e-4,

    "feature":"title",
    "use_chat_template":false,

    "bf16":true,
    "seed": 42,
    "save_steps":20,
    "eval_steps":20,
    "save_only_model":true,
    "deepspeed":"configs/ds_zero_1.json"
}